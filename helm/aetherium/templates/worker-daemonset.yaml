{{- if and .Values.worker.enabled (eq .Values.worker.kind "DaemonSet") }}
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: {{ include "aetherium.fullname" . }}-worker
  labels:
    {{- include "aetherium.worker.labels" . | nindent 4 }}
spec:
  selector:
    matchLabels:
      {{- include "aetherium.worker.selectorLabels" . | nindent 6 }}
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        {{- include "aetherium.worker.selectorLabels" . | nindent 8 }}
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
    spec:
      {{- include "aetherium.imagePullSecrets" . | nindent 6 }}
      serviceAccountName: {{ include "aetherium.serviceAccountName" . }}

      # CRITICAL: Workers run on host network for VM networking
      hostNetwork: true
      hostPID: {{ .Values.worker.hostPID | default false }}
      dnsPolicy: ClusterFirstWithHostNet

      # Terminate gracefully - allow VMs to shutdown
      terminationGracePeriodSeconds: {{ .Values.worker.terminationGracePeriodSeconds | default 120 }}

      # Init container to verify host prerequisites and prepare rootfs template
      initContainers:
        - name: prepare-rootfs
          # Use the worker image which contains fc-agent binary
          image: {{ include "aetherium.worker.image" . }}
          imagePullPolicy: {{ .Values.worker.image.pullPolicy | default .Values.global.imagePullPolicy }}
          command:
            - /bin/sh
            - -c
            - |
              set -e
              echo "=== Aetherium Worker Initialization ==="
              echo ""

              # 1. Verify KVM
              echo "1. Checking KVM..."
              if [ ! -e /dev/kvm ]; then
                echo "ERROR: /dev/kvm not found - KVM not available"
                exit 1
              fi
              echo "   ✓ KVM device available"

              # 2. Verify kernel
              echo "2. Checking Firecracker kernel..."
              if [ ! -f /var/firecracker/vmlinux ]; then
                echo "ERROR: Kernel not found at /var/firecracker/vmlinux"
                exit 1
              fi
              echo "   ✓ Kernel image found"

              # 3. Verify base rootfs exists
              echo "3. Checking base rootfs..."
              if [ ! -f /var/firecracker/rootfs.ext4 ]; then
                echo "ERROR: Base rootfs not found at /var/firecracker/rootfs.ext4"
                exit 1
              fi
              echo "   ✓ Base rootfs image found"

              # 4. Prepare rootfs template (per-VM isolation)
              echo "4. Preparing rootfs template..."

              # Check if template already exists
              if [ ! -f /var/firecracker/rootfs-template.ext4 ]; then
                echo "   Creating rootfs template from base..."
                cp /var/firecracker/rootfs.ext4 /var/firecracker/rootfs-template.ext4
                echo "   ✓ Template created"
              else
                echo "   ✓ Template already exists"
              fi

              # 5. Install fc-agent in template (idempotent)
              echo "5. Installing fc-agent in template..."
              MOUNT_POINT="/tmp/rootfs-mount"
              mkdir -p "$MOUNT_POINT"
              mount -o loop /var/firecracker/rootfs-template.ext4 "$MOUNT_POINT"

              if [ -f "$MOUNT_POINT/usr/local/bin/fc-agent" ] && \
                 [ -f "$MOUNT_POINT/etc/systemd/system/fc-agent.service" ]; then
                echo "   ✓ fc-agent already installed in template"
              else
                echo "   Installing fc-agent..."

                # Copy fc-agent binary from this container
                cp /usr/local/bin/fc-agent "$MOUNT_POINT/usr/local/bin/fc-agent"
                chmod +x "$MOUNT_POINT/usr/local/bin/fc-agent"

                # Create systemd service
                cat > "$MOUNT_POINT/etc/systemd/system/fc-agent.service" <<'EOF'
              [Unit]
              Description=Firecracker Agent
              After=network.target

              [Service]
              Type=simple
              ExecStart=/usr/local/bin/fc-agent
              Restart=always
              RestartSec=3
              StandardOutput=journal
              StandardError=journal

              [Install]
              WantedBy=multi-user.target
              EOF

                # Enable service
                mkdir -p "$MOUNT_POINT/etc/systemd/system/multi-user.target.wants"
                ln -sf /etc/systemd/system/fc-agent.service \
                  "$MOUNT_POINT/etc/systemd/system/multi-user.target.wants/fc-agent.service"

                echo "   ✓ fc-agent installed and enabled"
              fi

              umount "$MOUNT_POINT"

              # 6. Verify template integrity
              echo "6. Verifying template integrity..."
              e2fsck -f -y /var/firecracker/rootfs-template.ext4 || true
              echo "   ✓ Template filesystem check complete"

              # 7. Mark template as read-only (prevent accidental modification)
              chmod 444 /var/firecracker/rootfs-template.ext4
              echo "   ✓ Template marked read-only"

              # 8. Self-healing: Cleanup orphaned VM rootfs files
              echo "8. Cleaning up orphaned VM rootfs files..."
              ORPHANED_COUNT=0
              for rootfs in /var/firecracker/rootfs-vm-*.ext4; do
                if [ -f "$rootfs" ]; then
                  VM_ID=$(echo "$rootfs" | sed 's/.*rootfs-vm-\(.*\)\.ext4/\1/')
                  SOCKET="/run/aetherium/aetherium-vm-${VM_ID}.sock"

                  # If socket doesn't exist, rootfs is orphaned
                  if [ ! -S "$SOCKET" ]; then
                    echo "   Cleaning orphaned rootfs: $rootfs"
                    rm -f "$rootfs"
                    ORPHANED_COUNT=$((ORPHANED_COUNT + 1))
                  fi
                fi
              done

              if [ $ORPHANED_COUNT -eq 0 ]; then
                echo "   ✓ No orphaned rootfs files found"
              else
                echo "   ✓ Cleaned $ORPHANED_COUNT orphaned rootfs file(s)"
              fi

              echo ""
              echo "=== Initialization Complete ==="
              echo "Worker is ready to spawn VMs with per-VM rootfs isolation"
          securityContext:
            privileged: true
          volumeMounts:
            - name: dev-kvm
              mountPath: /dev/kvm
            - name: firecracker-data
              mountPath: /var/firecracker

      containers:
        - name: worker
          image: {{ include "aetherium.worker.image" . }}
          imagePullPolicy: {{ .Values.worker.image.pullPolicy | default .Values.global.imagePullPolicy }}

          # CRITICAL: Privileged for KVM, TAP devices, bridges
          securityContext:
            privileged: true
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
                - SYS_RESOURCE

          ports:
            - name: health
              containerPort: 8081
              hostPort: 8081
              protocol: TCP

          env:
            - name: CONFIG_PATH
              value: /etc/aetherium/config.yaml
            # Node identification
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            # Consul for service discovery
            {{- if .Values.consul.enabled }}
            - name: CONSUL_ADDR
              value: "{{ include "aetherium.fullname" . }}-consul:8500"
            {{- else if .Values.consul.external.addr }}
            - name: CONSUL_ADDR
              value: {{ .Values.consul.external.addr | quote }}
            {{- end }}
            # Worker port for health checks
            - name: WORKER_PORT
              value: "8081"
            # Database credentials
            {{- if .Values.postgresql.enabled }}
            - name: POSTGRES_HOST
              value: "{{ .Release.Name }}-postgresql"
            - name: POSTGRES_USER
              value: {{ .Values.postgresql.auth.username | quote }}
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ .Release.Name }}-postgresql
                  key: password
            - name: POSTGRES_DB
              value: {{ .Values.postgresql.auth.database | quote }}
            {{- else }}
            - name: POSTGRES_HOST
              value: {{ .Values.postgresql.external.host | quote }}
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.postgresql.external.existingSecret }}
                  key: {{ .Values.postgresql.external.secretKeys.username }}
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.postgresql.external.existingSecret }}
                  key: {{ .Values.postgresql.external.secretKeys.password }}
            {{- end }}
            # Redis
            - name: REDIS_ADDR
              value: "{{ include "aetherium.redis.host" . }}:{{ include "aetherium.redis.port" . }}"
            {{- with .Values.worker.env }}
            {{- toYaml . | nindent 12 }}
            {{- end }}

          volumeMounts:
            - name: config
              mountPath: /etc/aetherium
              readOnly: true
            # KVM device for Firecracker
            - name: dev-kvm
              mountPath: /dev/kvm
            # Vsock for VM communication
            - name: dev-vhost-vsock
              mountPath: /dev/vhost-vsock
            # Firecracker assets (kernel, rootfs, binary)
            - name: firecracker-data
              mountPath: /var/firecracker
            # VM sockets and runtime data
            - name: vm-run
              mountPath: /run/aetherium
            # Logs
            - name: logs
              mountPath: /var/log/aetherium

          livenessProbe:
            httpGet:
              path: /health
              port: health
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /health
              port: health
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3

          resources:
            {{- toYaml .Values.worker.resources | nindent 12 }}

      volumes:
        - name: config
          configMap:
            name: {{ include "aetherium.configMapName" . }}
        # Host devices
        - name: dev-kvm
          hostPath:
            path: /dev/kvm
            type: CharDevice
        - name: dev-vhost-vsock
          hostPath:
            path: /dev/vhost-vsock
            type: CharDevice
        # Firecracker assets from host
        - name: firecracker-data
          hostPath:
            path: /var/firecracker
            type: Directory
        # Runtime directory for VM sockets
        - name: vm-run
          hostPath:
            path: /run/aetherium
            type: DirectoryOrCreate
        # Logs
        - name: logs
          hostPath:
            path: /var/log/aetherium
            type: DirectoryOrCreate

      # CRITICAL: Only schedule on KVM-enabled bare-metal nodes
      nodeSelector:
        {{- if .Values.worker.nodeSelector }}
        {{- toYaml .Values.worker.nodeSelector | nindent 8 }}
        {{- else }}
        aetherium.io/kvm-enabled: "true"
        {{- end }}

      {{- with .Values.worker.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}

      tolerations:
        # Tolerate worker-dedicated nodes
        - key: "aetherium.io/worker"
          operator: "Exists"
          effect: "NoSchedule"
        {{- with .Values.worker.tolerations }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
{{- end }}
